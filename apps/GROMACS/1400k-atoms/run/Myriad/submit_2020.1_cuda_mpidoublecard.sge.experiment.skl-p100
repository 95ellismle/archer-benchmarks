#!/bin/bash --login
#$ -S /bin/bash
#$ -l h_rt=6:0:0
#$ -N mpitwogpuGromacsBench
#$ -pe mpi 36
#$ -cwd
#$ -l mem=4000M
#$ -t 1-10
#$ -l gpu=2

#$ -ac allow=J

set -e

cat $0

NUM_NODES=1

BENCHDIR=/home/uccaoke/Scratch/archer-benchmarks/apps/GROMACS/1400k-atoms

module purge
module load beta-modules
module load gcc-libs/7.3.0
module load compilers/gnu/7.3.0
module load gerun
module load cuda/10.1.243/gnu-7.3.0
module load mpi/openmpi/3.1.4/gnu-7.3.0
module load gromacs/2020.1/cuda-10.1

MDRUN="mdrun_cuda_mpi"

casename=nc2-cubic-md
timestamp=$(date '+%Y%m%d%H%M')

INPUTDIR=$BENCHDIR/run
TPRFILE=benchmark.tpr

WORKDIR=`pwd`/Scratch.mpi.2GPU.${NSLOTS}.${SGE_TASK_ID}


#number of OpenMP threads per task is calculated 
export OMP_NUM_THREADS=36


#number of PME ranks - a major source of load imbalance!
#NPME=42

rm -rf $WORKDIR
mkdir -p $WORKDIR
cd $WORKDIR

cp $INPUTDIR/$TPRFILE .

export OMP_NUM_THREADS=18

date
gerun $MDRUN -s $TPRFILE -deffnm $casename -nb gpu -pme gpu -pmefft gpu -ntomp $OMP_NUM_THREADS -noconfout -dlb yes -npme 1
date

echo
echo $casename.log
echo "------------"
resfile="${casename}_${NUM_NODES}nodes_${timestamp}.log"
mv $casename.log ${BENCHDIR}/run/mpi.2gpu.${NSLOTS}.${SGE_TASK_ID}.log


